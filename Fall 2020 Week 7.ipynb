{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fall 2020 Week 7.ipynb","provenance":[],"collapsed_sections":["3r1C6kMVKkoA"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RAFyeylt9Ryi"},"source":["Before we start: please fill out our weekly attendance form! https://forms.gle/njrwXR9r416yzXnn7"]},{"cell_type":"markdown","metadata":{"id":"f5r4AgVg959D"},"source":["Fall quarter outline:\n","\n","* Week 3: Introduction\n","* Week 4: Data retrieval and preparation\n","* Week 5: Exploratory data analysis (EDA)\n","* Week 6: Modeling and machine learning, part 1\n","* **Week 7: Modeling and machine learning, part 2**\n","    * Recap of last week\n","    * Supervised learning, continued\n","        * Classification algorithms\n","        * Regression algorithms\n","* *Week 8: Thanksgiving*\n","* Week 9: Neural networks "]},{"cell_type":"markdown","metadata":{"id":"G78L3FkiDSwE"},"source":["<img src=\"https://miro.medium.com/max/1200/1*eE8DP4biqtaIK3aIy1S2zA.png\" width=\"800\">"]},{"cell_type":"markdown","metadata":{"id":"3r1C6kMVKkoA"},"source":["### Mounting Google Drive"]},{"cell_type":"markdown","metadata":{"id":"U-Q3Joa-J66y"},"source":["Don't worry about this; this bit of code is only necessary because we're working in Google Colab instead of Jupyter Notebook. I'm mounting Google Drive to this notebook so that I can access files that are stored there."]},{"cell_type":"code","metadata":{"id":"YPorJK5vGlJV"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KpaOn0gUIUNT"},"source":["%cd \"drive/My Drive/DSU 2020-2021/Curriculum/Datasets\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kThxhvpYKSqf"},"source":["!ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GuYO2c-Dt_Yd"},"source":["# Recap"]},{"cell_type":"markdown","metadata":{"id":"LBkNjeQHx8b8"},"source":["## Overview"]},{"cell_type":"markdown","metadata":{"id":"77FP9r2teKpK"},"source":["To recap last week, **machine learning** is done through machine learning **algorithms**, all of which use **training data** to improve their performance on a specific task.\n","\n","<!-- Per the editor of *Introducing Data Science*, \"Machine learning is the process by which a computer can work more accurately as it collects and learns from the data it is given.\" -->"]},{"cell_type":"markdown","metadata":{"id":"Mm5mkadLy7z3"},"source":["The two main types of machine learning are **unsupervised** learning and **supervised** learning. \n","* *Unsupervised* &rarr; *unlabeled* training data\n","* *Supervised* &rarr; *labeled* training data\n","\n","In general, most of our problems will be supervised."]},{"cell_type":"markdown","metadata":{"id":"02EQsG6pwwOh"},"source":["Supervised learning can be further broken down into **two categories**:\n","* **Regression** &rarr; continuous (numeric) target variable\n","* **Classification** &rarr; discrete (categorical) target variable\n"]},{"cell_type":"markdown","metadata":{"id":"tgISZvQQvqLc"},"source":["Last week, we went through two examples of unsupervised algorithms: PCA (for signal separation), and K-means (for clustering).\n","\n","This week, we're going to focus on several **supervised learning algorithms**, namely:\n","* For **classification**: KNN, naive Bayes, decision trees, logistic regression\n","* For **regression**: (KNN, decision trees,) linear models, neural networks"]},{"cell_type":"markdown","metadata":{"id":"qAVoCj4lzB8T"},"source":["However, keep in mind that **we are not covering every single ML algorithm!** There are wayyy more algorithms than we have time to touch on, and different models perform better on different data. You can find a pretty comprehensive list at the [scikit-learn user guide](https://scikit-learn.org/stable/user_guide.html)."]},{"cell_type":"markdown","metadata":{"id":"wiAqlY5pBKo-"},"source":["## The scikit-learn package"]},{"cell_type":"markdown","metadata":{"id":"mxZ7i1cQ0GSU"},"source":["Speaking of scikit-learn, let's recap how to train <strike>your dragon</strike> a supervised model using it."]},{"cell_type":"markdown","metadata":{"id":"84z9-1oSLdiQ"},"source":["Once you've decided which algorithm to use, you import it from the correct `sklearn` module and initialize it using its constructor, specifying optional **hyperparameters** &mdash; parameters whose value control the training process.\n","\n","```\n","from sklearn.[module] import [Estimator]\n","\n","model = Estimator([optional hyperparameters])\n","```\n","\n","<span style=\"font-size:8pt;\">\n","(You should read the documentation as needed to understand each model's parameters and their default values.)\n","</span>"]},{"cell_type":"markdown","metadata":{"id":"SLPfN0tTLfuU"},"source":["After initializing the model, all you have to do to train the model is call the `.fit()` method with your training data:\n","\n","```\n","model.fit(X)\n","```\n","\n","<span style=\"font-size:8pt;\">\n","(Note: Per the <a href=\"https://scikit-learn.org/stable/faq.html\">sklearn FAQ</a>, <code>X</code> must be numeric data stored as a <code>numpy</code> array. This includes <code>pandas.DataFrame</code> since they're convertible to <code>numpy</code> arrays.)\n","</span>"]},{"cell_type":"markdown","metadata":{"id":"SiXitDwjRYk_"},"source":["Then, for supervised models, you call the `.predict()` method to generate predictions from the training data (or new data):\n","\n","```\n","predictions = model.predict(X)\n","```"]},{"cell_type":"markdown","metadata":{"id":"-hFDIa-YOQAI"},"source":["And that's it! Each model will also have a number of additional methods and attributes that you can access, but again, you'll have to refer to the documentation as you go."]},{"cell_type":"markdown","metadata":{"id":"4Ku1M-5wzIuI"},"source":["## Fundamental concepts"]},{"cell_type":"markdown","metadata":{"id":"ebV3eI0KBP80"},"source":["Mathematically speaking, **the goal of supervised learning** is to take a set of *training observations* $\\{x_1, x_2, \\dots, x_n\\}$ and their *target* labels $\\{t_1, t_2, \\dots, t_n\\}$ and **learn a function** $y(x)$ such that $y(x_i) \\approx t_i$ for all the training data."]},{"cell_type":"markdown","metadata":{"id":"QatId26kyteC"},"source":["### Train-test split"]},{"cell_type":"markdown","metadata":{"id":"D2Yb-Uo4ys5B"},"source":["To see how well our model **generalizes** to new data, we can use **train-test split**, in which we randomly split our data into *training* and *testing* subsets (usually in a 80/20 or 70/30 ratio). The model *learns* from the training set, and to estimate its *generalization error*, we make predictions on the test set and compare them with the actual targets."]},{"cell_type":"markdown","metadata":{"id":"hPBXQs1ywBgd"},"source":["*New this week*: \n","\n","I'll demonstrate how to split data using the `train_test_split()` function from `sklearn.model_selection`. But, it literally takes one line of code:\n","\n","```\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = __)\n","```\n","\n","First, importing the `iris` dataset:"]},{"cell_type":"code","metadata":{"id":"MQ8hKEyOwe0l"},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","\n","iris = load_iris()\n","X = pd.DataFrame(iris.data, columns=iris.feature_names)\n","y = iris.target\n","\n","X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hxAF70UHxHrY"},"source":["y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GQlFNC0VxaK9"},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8) # 80/20 split\n","\n","print(\"X.shape:      \", X.shape)\n","print(\"X_train.shape:\", X_train.shape)\n","print(\"X_test.shape: \", X_test.shape)\n","print(\"\")\n","print(\"y.shape:      \", y.shape)\n","print(\"y_train.shape:\", y_train.shape)\n","print(\"y_test.shape: \", y_test.shape)\n","print(\"\")\n","\n","X_train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_2TR_lnzy2ZA"},"source":["### Overfitting and underfitting, bias-variance tradeoff"]},{"cell_type":"markdown","metadata":{"id":"3rvFlVy1VFsw"},"source":["<img src=\"https://www.educative.io/api/edpresso/shot/6668977167138816/image/5033807687188480\" width = \"700px\">"]},{"cell_type":"markdown","metadata":{"id":"KwZatT8W0hlw"},"source":["The purpose of the train-test split is to see if our model **overfits** the data, **underfits** the data, or fits the data well. In general:\n","\n","* *Underfitting* &rarr; make the model *more complex* (more features/parameters)\n","* *Overfitting* &rarr; make the model *simpler* (fewer features/parameters; regularization)\n"]},{"cell_type":"markdown","metadata":{"id":"AOFfrPFIYl6p"},"source":["<img src=\"https://www.learnopencv.com/wp-content/uploads/2017/02/Bias-Variance-Tradeoff-In-Machine-Learning-1.png\" width=\"600px\">"]},{"cell_type":"markdown","metadata":{"id":"cLRaPJ6vcJBD"},"source":["To understand overfitting/underfitting better, we introduced the idea of **bias-variance tradeoff**. Basically, no matter what ML algorithm we choose, we will always need to find a balance between its *bias* and its *variance*.\n","\n","* Model **bias** is *training error* (MSE for regression, or misclassification rate for classification). High bias &rarr; underfitting.\n","* Model **variance** is how *sensitive* a model is to *noise* in the training data. High variance &rarr; overfitting."]},{"cell_type":"markdown","metadata":{"id":"JZVENPhmVMsU"},"source":["### Cross-validation"]},{"cell_type":"markdown","metadata":{"id":"XZpW3FpQec1w"},"source":["To select the optimal hyperparameters for our model, we can use **cross-validation**. Cross-validation is also a more robust strategy to determine how well our model generalizes."]},{"cell_type":"markdown","metadata":{"id":"S-ZIJbJtVOtm"},"source":["To perform cross-validation, we divide our data into **three groups**: training, validation, and testing and then follow these steps:\n","\n","* Use the *training set* to actually train the model using different combinations of *hyperparameters*. \n","* Calculate the model's *validation error* using the *validation set*. Typically, we'll choose the combination of hyperparameters which minimizes the validation error.\n","* Estimate the model's generalization error using the *test set*."]},{"cell_type":"markdown","metadata":{"id":"uBrRYLcshMuC"},"source":["The technique we usually use is **K-fold cross-validation**:\n","* Randomly divide the training observations into $K$ groups (or folds). \n","* For each iteration, one group is selected as the validation set, while the $K-1$ others are used as the training set.\n","\n","<!-- * **Leave-one-out cross-validation (LOOCV)**: the same as K-fold, except we take $K=N$ (# of training observations). For each iteration, just one observation is selected as the validation set, and we repeat over the entire training set. -->\n","\n","There's also **leave-one-out cross-validation (LOOCV)** in which we take $K=N$ (the number of training observations), but this is very computationally expensive on large datasets."]},{"cell_type":"markdown","metadata":{"id":"irfOWtaycFw4"},"source":["*New this week:*\n","\n","I'll demonstrate how to perform K-fold cross validation using functions from `sklearn.model_selection`. But we can't do that without a model first, so I'll run it at the end of the first example below (KNN)."]},{"cell_type":"markdown","metadata":{"id":"FUZdcz-XMolN"},"source":["# Classification algorithms"]},{"cell_type":"markdown","metadata":{"id":"uQa8wh-CrneY"},"source":["## K Nearest Neighbors (KNN)"]},{"cell_type":"markdown","metadata":{"id":"M5WZ9fbaNRiQ"},"source":["With the **K-nearest neighbors (KNN) classifier**, we classify a point based on the classes of the $K$ points closest in distance to it. \n","\n","Distance here is usually the Euclidean distance: $d = \\sqrt{(x_2-x_1)^2 + (y_2-y_1)^2}$.\n","\n","<!-- <img src=\"https://www.oreilly.com/library/view/hands-on-recommendation-systems/9781788993753/assets/1c808a35-3c9d-4bbe-a6ae-e858a3961159.png\" width = \"800px\"> -->\n","\n","We'll focus on a 2D example, but we could use more than two features and the algorithm would factor this into the distance calculations, e.g. the distance between 2 points in 3D is $d = \\sqrt{(x_2-x_1)^2 + (y_2-y_1)^2 + (z_2-z_1)^2}$.\n","\n","The algorithm goes as follows:\n","\n","\n","1.   Calculate the distance between the point we wish to classify and all other points in the dataset. (KNN doesn't require splitting the data into training/testing sets.)\n","2.   Order these distances.\n","3.   Count how many of the *K* nearest neighbors belong to each class. (We tell the algorithm what number to use for *K*.)\n","4.   Whichever class is the most common among the *K* nearest neighbors is the class we assign the new point to.\n","\n","<img src = \"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final1_ibdm8a.png\" height = \"450px\" width = \"600px\">\n","\n","You can see that how we classify the new point depends on what value we choose for *K*.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RvOqu1KTShAu"},"source":["### Coding KNN"]},{"cell_type":"code","metadata":{"id":"Oc5uKJ2qSrL9"},"source":["# load the iris data set and pandas library\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","\n","iris = load_iris()\n","df = pd.DataFrame(iris.data, columns=iris.feature_names)\n","\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RBJ9aYjFYE4r"},"source":["# 3 classes (0 = \"setosa\", 1 = \"versicolor\", 2 = \"virginica\")\n","iris.target, iris.target_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2eBnRucZYHlR"},"source":["# for simplicity's sake we'll only use sepal length and sepal width\n","X = df[['petal length (cm)', 'sepal width (cm)']]\n","y = iris.target\n","\n","# scale the data\n","from sklearn import preprocessing\n","\n","scaler = preprocessing.StandardScaler()\n","X_scaled = scaler.fit_transform(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BhPmeBenWr70"},"source":["# load and set up the KNN classifier w/ K=3\n","from sklearn.neighbors import KNeighborsClassifier\n","knn3 = KNeighborsClassifier(n_neighbors=3)\n","knn3.fit(X_scaled, y)\n","\n","# test it out with sepal length = 0.5 and sepal width = 2.6\n","prediction = knn3.predict([[0.5, 2.6]])\n","print(iris.target_names[prediction])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b2sQmZlZbChi"},"source":["# try the same but with K=6\n","knn6 = KNeighborsClassifier(n_neighbors=6)\n","knn6.fit(X_scaled, y)\n","prediction = knn6.predict([[0.5, 2.6]])\n","print(iris.target_names[prediction])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Px3QdD8idGKH"},"source":["# how did we do?\n","print(\"K=3, accuracy:\", knn3.score(X_scaled, y))\n","print(\"K=6, accuracy:\", knn6.score(X_scaled, y))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pUhhgu2CjvlE"},"source":["from sklearn.metrics import confusion_matrix\n","\n","print(\"K=3, confusion matrix:\")\n","print(confusion_matrix(y, knn3.predict(X_scaled)))\n","print(\"\")\n","print(\"K=6, confusion matrix:\")\n","print(confusion_matrix(y, knn6.predict(X_scaled)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BpvObWFbRMyu"},"source":["### How do we choose *k*?\n","\n","*   Higher values of *k* require more computation, while lower values of k make the algorithm more error-prone.\n","*   We typically choose *k* to be an odd number to avoid ties between classes.\n","*   The rule of thumb is if we have *n* observations, then *k* should be $\\sqrt{n}$."]},{"cell_type":"markdown","metadata":{"id":"FYlzYHz5DkqJ"},"source":["**Quiz time!**\n","\n","<img src = \"https://miro.medium.com/max/754/1*VZCDAcvMqlU3bPlb5BfPug.png\" width = \"400px\">\n","\n","**Which class do we assign the new point (the red one) if we choose k=3?**  \n","**A**. Class A  \n","**B**. Class B    \n","**How about if k=6?**"]},{"cell_type":"markdown","metadata":{"id":"iR_SaUM_kxHN"},"source":["### Cross validation, cont."]},{"cell_type":"markdown","metadata":{"id":"_LOvYTlRlJFU"},"source":["Another way to choose the $K$ for KNN is through K-fold cross-validation! To do that in `sklearn`, we can use the `cross_val_score()` function from `sklearn.model_selection`:"]},{"cell_type":"code","metadata":{"id":"IM6-nH_8m4FA"},"source":["from sklearn.model_selection import cross_val_score\n","\n","mean_fold_errors = []\n","\n","for num_neighbors in range(1, 20):\n","    np.random.seed(0)   # to ensure we use the same random folds\n","\n","    knn = KNeighborsClassifier(num_neighbors)\n","    fold_errors = 1 - cross_val_score(knn, X_scaled, y, cv = 5)   # 5-fold CV\n","    mean_fold_errors.append(np.mean(fold_errors))\n","\n","    print(\"\")\n","    print(\"Num neighbors =\", num_neighbors)\n","    print(\"K-fold CV errors:\", np.round(fold_errors, 3))\n","    print(\"K-fold CV mean error:\", np.round(mean_fold_errors[-1], 3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jnmDOD6WuFWu"},"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(range(1, 20), mean_fold_errors)\n","plt.xticks(range(1, 20, 2))\n","plt.xlabel('$K$ (Num Neighbors)')\n","plt.ylabel('Mean CV Error')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uMsKJ08Jv9aS"},"source":["## Naive Bayes"]},{"cell_type":"markdown","metadata":{"id":"CEokycZfdgzB"},"source":["The **Naive Bayes classifier** requires knowing a bit about Bayes Rule:\n","\n","<img src = \"https://miro.medium.com/max/1994/1*CnoTGGO7XeUpUMeXDrIfvA.png\" height = \"400px\" width = \"500 px\">\n","\n","Bayes’ Rule is a law of probability that describes the relationship between the probability of an event occurring, given a certain piece of information (**the “posterior”**), and the probability of the event occurring without knowledge of this information (**the “prior”**).\n","\n","**Terminology check:** if **probability** is the chance of observing some data given some parameter assumption (e.g. if mean = 0 and sd = 1, what are the chances we observe 0.5?), then **likelihood** is the chance that the parameter is some value given that we've observed some data (e.g. if we observe 0.5, what are the chances that mean = 0 and sd = 1?).\n","\n","Back to the Bayes Rule formula. In Naive Bayes we can ignore the denominator P(B) because it's a constant and all that matters is that the Posterior is **proportional** to  Likelihood * Prior. This makes for easier calculations!\n","\n","The classifier is called \"naive\" because we assume a) our observations are independent, and b) our features are normally distributed. This is called **Gaussian Naive Bayes**; there are other variations of this algorithm, but we'll focus on the Gaussian case here.\n","\n","In the context of the Iris data set, assume we're looking for the probability that a new obsevation is a setosa. Bayes Rule then looks like: $$P(X = \\text{setosa} \\mid \\text{some observed data } \\mu \\text { and } \\sigma) \\propto P(\\text{observing } \\mu \\text { and } \\sigma \\mid X = \\text{setosa}) * P(\\text{any observation is setosa})$$\n","\n","As you might have guessed, Naive Bayes returns a probability that an observation returns to a class, so even though it's a categorical classifier, it's still probabilistic."]},{"cell_type":"markdown","metadata":{"id":"6bipiyynaBJN"},"source":["### Coding Naive Bayes"]},{"cell_type":"code","metadata":{"id":"6aJX3i9taKoh"},"source":["from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import train_test_split\n","\n","X = df\n","y = iris.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n","\n","# train the model\n","nb = GaussianNB()\n","nb.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JAmbXA_I_6wQ"},"source":["# we don't actually have to set the prior probabilities -- GaussianNB() adjusts them to the data\n","nb.class_prior_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eN3qzVeF_4k0"},"source":["# test the model\n","y_predicted = nb.predict(X_test)\n","print(y_predicted)\n","print(\"\")\n","\n","# the underlying list of probabilities\n","print(np.round(nb.predict_proba(X_test), 2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TLEFmAj1-d7P"},"source":["# how did we do?\n","nb.score(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xcZiR8c0j5bj"},"source":["When should we use Naive Bayes?\n","\n","**Pros:**\n","*   Super quick\n","*   Relatively simple\n","*   Doesn't require a ton of data to make a decent clasfication\n","\n","**Cons:**\n","*    Have to assume data follows a distribution\n","*    Have to assume independence\n","*    Only performs well when classes are very separated"]},{"cell_type":"markdown","metadata":{"id":"EGAo9wrXCidb"},"source":["## Decision trees"]},{"cell_type":"markdown","metadata":{"id":"82Uysne7xmpX"},"source":["**Decision trees** are another method we can use to classify data. They're best explained by providing a simple example:"]},{"cell_type":"markdown","metadata":{"id":"v8otpLB984ZY"},"source":["<img src = \"https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2017/07/Decision-Trees-Example.png\" width = \"450 px\">\n","\n","We see that a decision tree consists of several *nodes* connected by *branches*. At each node, we make a \"decision\" based off a certain criterion (e.g. \"Is it raining?\"), and we follow these nodes and branches until we reach a *terminal node*. To classify an observation, we start at the top and use the criteria contained in each node to decide which branch to follow."]},{"cell_type":"markdown","metadata":{"id":"grs6wbI_9iMH"},"source":["<span style=\"font-size:8pt;\">\n","[Mathematically, the algorithm determines which criterion is \"best\" at each node by minimizing the <i>impurity</i> resulting from all possible criteria (each category for categorical predictors, thresholds for numeric predictors). The most common impurity measure is <i>Gini impurity</i>: \n","<br>\n","$\\sum\\limits_k p_{k} (1 - p_{k}) = 1 - \\sum\\limits_k p_{k}^2$\n","<br>\n","where each $k$ represents one of the target classes of the data, and $p_k$ is the proportion of the data belonging to that class.]\n","</span>\n"]},{"cell_type":"markdown","metadata":{"id":"LzmiyJws9f0O"},"source":["Decision trees have several pros and cons.\n","\n","**Pros:**\n","* They are simple and easy to understand\n","* It's very quick to classify new observations\n","* They can handle numeric and categorical predictors\n","\n","**Cons:**\n","* Computationally expensive to train for large datasets\n","* They are often inaccurate\n","* They are prone to overfitting\n","\n","However, we can address two of these disadvantages by using an **ensemble method** called **Random Forest**, which trains many decision trees using random splits of the data and averaging the results."]},{"cell_type":"markdown","metadata":{"id":"XPl0lKXMB36a"},"source":["### Coding Decision Trees"]},{"cell_type":"code","metadata":{"id":"8vX4t9HXB-zW"},"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","X = df\n","y = iris.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n","\n","# train the model\n","dt = DecisionTreeClassifier()\n","dt.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OnMaEIjOEiu_"},"source":["Another advantage of decision trees is that we can actually plot the model itself:"]},{"cell_type":"code","metadata":{"id":"D-G5Kmy7CMBu"},"source":["from sklearn.tree import plot_tree\n","\n","plt.figure(figsize=(10,8))\n","plot_tree(dt, feature_names=iris.feature_names, class_names=iris.target_names,\n","          filled=True, fontsize=9)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rw8ZehDEC7Np"},"source":["# how did we do?\n","dt.score(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FQy1RLq5Crpm"},"source":["## Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"FeW1PMDfxxOK"},"source":["**Logistic regression**, despite its name, is a linear model for classification. It works by assuming that you can approximate the probability that an observation belongs to a certain class with the **sigmoid (or logistic) function**: $\\sigma(x) = \\dfrac{1}{1+e^{-x}}$."]},{"cell_type":"code","metadata":{"id":"djduV2DySxe_"},"source":["x = np.linspace(-8, 8)\n","y = 1 / (1 + np.exp(-x))\n","\n","plt.plot(x, y)\n","plt.grid(True)\n","plt.xlabel('x')\n","plt.ylabel('$\\sigma(x)$')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wNQ5U5rESSOz"},"source":["To classify an observation $X = (x_1, \\dots, x_n)$, we plug in a linear combination of the predictors into the sigmoid function: $y(x) = \\sigma(b_0 + b_1 x_1 + \\dots + b_n x_n)$, and take the class for which the predicted probability is greatest."]},{"cell_type":"markdown","metadata":{"id":"yHaeZ51AVNOj"},"source":["<span style=\"font-size:8pt;\">\n","[Mathematically, the algorithm works by minimizing the <i>cross-entropy</i>. In the binary case, this is: \n","<br>\n","$-\\sum\\limits_{i=1}^{N} \\{t_n \\ln y(x_n) + (1-t_n) \\ln (1-y(x_n))\\}$\n","<br>\n","since the predicted probabilities between the two classes sum to 1. The model uses <i>gradient descent</i> to solve for the optimal solution for the coefficients.]\n","</span>\n"]},{"cell_type":"markdown","metadata":{"id":"CqZfutOVT21w"},"source":["### Coding Logistic Regression"]},{"cell_type":"code","metadata":{"id":"vHo3B3dST2XC"},"source":["from sklearn.linear_model import LogisticRegression\n","\n","X = df\n","y = iris.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n","\n","# train the model\n","lr = LogisticRegression(max_iter=200)   # (I only specify max_iter b/c the default max isn't enough here)\n","lr.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OfK-6Js7Xzu7"},"source":["# the model's coefficients (b0, b1, ..., bn)\n","# note that there are three sets of coeffs, since there are three classes\n","lr.intercept_, lr.coef_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UdNvNsJcUanV"},"source":["# the predicted probabilities of each class\n","np.round(lr.predict_proba(X_train), 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UOtCKIvdXq4I"},"source":["# how did we do?\n","lr.score(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9T0-_yw1260k"},"source":["# Regression algorithms"]},{"cell_type":"markdown","metadata":{"id":"IboGUEDX2dpa"},"source":["## KNN and decision trees"]},{"cell_type":"markdown","metadata":{"id":"x2wge0lTEy0B"},"source":["These two algorithms again? Yup! \n","\n","With some small modifications, each of these classification algorithms (and some others) can also be used for regression. We won't explain them in much more detail, but here's a code example of each one:"]},{"cell_type":"code","metadata":{"id":"F8mcV5TVa7X-"},"source":["from sklearn.datasets import load_diabetes\n","\n","diabetes = load_diabetes()\n","X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n","y = diabetes.target # a measure of disease progression one year after baseline\n","\n","X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Co5HOR3FbAvg"},"source":["y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kBsjoPf1btQR"},"source":["As with classification, we want to scale our data before applying KNN. Aside from calling `StandardScaler()` and KNN separately, we can also combine them using an `sklearn` `Pipeline`:"]},{"cell_type":"code","metadata":{"id":"hPq5BmkhbLec"},"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.pipeline import make_pipeline\n","\n","# split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","\n","# fit the model\n","knr = make_pipeline(StandardScaler(), KNeighborsRegressor())\n","knr.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ckQrkebchEJ"},"source":["knr.score(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zmaaIda0dI3t"},"source":["from sklearn.tree import DecisionTreeRegressor\n","\n","dtr = DecisionTreeRegressor()\n","dtr.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LZks41VCdWgM"},"source":["plot_tree(dtr) # this might take a while...\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WTAekkn4dnBQ"},"source":["dtr.score(X_test, y_test) # the model's R^2, not good"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H5fAD7jxahnV"},"source":["## Linear regression"]},{"cell_type":"markdown","metadata":{"id":"PBfs6_Rmak93"},"source":["The goal of **linear regression** is to model the relationship between predictor variables (inputs) and a response variable (output). The equation we use to model this relationship follows the form:"]},{"cell_type":"markdown","metadata":{"id":"OiAsxKJGanfJ"},"source":["<img src= \"https://csharpcorner-mindcrackerinc.netdna-ssl.com/article/linear-regression2/Images/f_MLR.png\" width = \"500px\">\n"]},{"cell_type":"markdown","metadata":{"id":"Vh6MmIAl3Adl"},"source":["How do we find this equation? In other words, how do we find the coefficients $\\beta_0,...,\\beta_n?$ The goal is to minimize our model's error, or **loss**. A common loss function in linear regression is\n","\n","$$L = \\sum_{i=1}^{n} (y_i-\\hat{y_i})^2$$ where we add up the squares of the **residuals** or the differences between our observed values ($y_i$) and the values predicted by our regression equation ($\\hat{y_i}$).\n","\n","We want to minimize this loss function. A little calculus can get us the answer, but since we won't ever actually have to calculate the regression equation by hand, we don't need to do the calculus here. Just remember that **the \"best\" linear model is the one that minimizes the loss function.**"]},{"cell_type":"markdown","metadata":{"id":"9EerKV7JWL1G"},"source":["### How do we measure the strength of a linear model?\n","\n","Straightforward: take the average of the loss function from earlier. We call this the **mean squared error (MSE)**.\n","\n","$$MSE = \\frac{1}{n}\\sum_{i=1}^{n} (y_i-\\hat{y_i})^2$$\n","\n","In general, the lower the MSE the stronger the model. (But if it's too low we might be overfitting!)"]},{"cell_type":"markdown","metadata":{"id":"78SmBx6iXz5c"},"source":["### Coding linear regression"]},{"cell_type":"code","metadata":{"id":"UngTlj1oX2rD"},"source":["# once again we load the appropriate dependencies from scikit learn\n","# we will also load scikit learn's diabetes dataset\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import load_diabetes\n","\n","diabetes = load_diabetes()\n","\n","X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n","y = diabetes.target # a measure of disease progression one year after baseline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iVc7gVHo1hA-"},"source":["# split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","\n","# fit the model\n","lm = LinearRegression()\n","lm.fit(X_train, y_train)\n","\n","# let's see what coefficients the model chose\n","lm.coef_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6SqGZkmL3bSi"},"source":["# and the intercept\n","lm.intercept_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"82cy8B4m33uf"},"source":["# let's see what the model predicts for the test data\n","lm.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OR7MnPcg4BFh"},"source":["# let's plot the predicted y values vs the actual y values\n","import numpy as np\n","import matplotlib.pyplot as plt\n","y_pred = lm.predict(X_test)\n","plt.plot(y_test, y_pred, \".\")\n","\n","x = np.linspace(0, 330, 100)\n","y = x\n","plt.plot(x, y)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jMbfqz1-5V51"},"source":["# how did we do?\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n","print(\"\")\n","print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SFtBeI7LEzz0"},"source":["### Variants of Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"Ar5AkEdz5-GX"},"source":["We can sometimes get better results by adding a **regularization** or **penalty** term.\n","\n","### Ridge Regression\n","\n","In **ridge regression** (or **L2 regularization**), rather than minimizing $\\sum_{i=1}^{n} (y_i-\\hat{y_i})^2$, we minimize $$\\sum_{i=1}^{n} (y_i-\\hat{y_i})^2 + \\lambda \\sum_{i=1}^{n}\\beta_{i}^{2}$$ where\n","*   $\\lambda$ is a penalty parameter we choose. The higher the value we choose, the smaller our coefficients end up being.\n","*   $\\beta_i$ are the coefficients selected by the model.\n","\n","Ridge regression favors smaller coefficients and reduces the overall variance of the model.\n","\n","### LASSO Regression\n","\n","In **LASSO regression** (or **L1 regularization**), we minimize $$\\sum_{i=1}^{n} (y_i-\\hat{y_i})^2 + \\lambda \\sum_{i=1}^{n} \\lvert \\beta_{i} \\rvert$$\n","\n","Lasso regression favors smaller coefficients and sometimes even makes them equal 0 (it eliminates variables), which is good if you want a simpler model but bad in that it increases the bias of your model."]},{"cell_type":"markdown","metadata":{"id":"Z-jlhppj2RW_"},"source":["## Neural networks"]},{"cell_type":"markdown","metadata":{"id":"C_-XISs6ZMfM"},"source":["Aside from the regression and classification methods we've already discussed, we can also use **neural networks**! But, we're probably running low on time now, and they're more complicated than the models we've already discussed, so we're saving them for next week."]},{"cell_type":"markdown","metadata":{"id":"bZ4ROwChYPqy"},"source":["# Anonymous feedback"]},{"cell_type":"markdown","metadata":{"id":"XguMyynDYRsc"},"source":["If you have any feedback for us, please let us know! The feedback form is completely anonymous, and we promise we'll take your suggestions into account for future presentations: https://forms.gle/C12vK71RJK6CraZv5"]},{"cell_type":"markdown","metadata":{"id":"CdzkrJIWhJyH"},"source":["# References"]},{"cell_type":"markdown","metadata":{"id":"YrlH0-VXhNby"},"source":["Throughout the quarter, we will mainly be drawing our material from the following sources. Most of your learning will be done through trial and error, so we strongly encourage you to experiment by running code that you write from scratch!\n","\n","For basic Python:\n","* The Python Tutorial: https://docs.python.org/3/tutorial/\n","* Basics of Python 3: https://www.learnpython.org/\n","* CodeAcademy Python 3 Course: https://www.codecademy.com/learn/learn-python-3\n","\n","For the rest of the quarter:\n","* Introducing Data Science: http://bedford-computing.co.uk/learning/wp-content/uploads/2016/09/introducing-data-science-machine-learning-python.pdf \n","* Python for Data Analysis: http://bedford-computing.co.uk/learning/wp-content/uploads/2015/10/Python-for-Data-Analysis.pdf \n","* Pandas user guide: https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html \n","* Sklearn user guide: https://scikit-learn.org/stable/user_guide.html "]}]}